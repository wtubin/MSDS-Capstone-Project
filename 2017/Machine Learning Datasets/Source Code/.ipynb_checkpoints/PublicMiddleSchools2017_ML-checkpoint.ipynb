{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create Public School Machine Learning Datasets\n",
    "** This program creates all the _ML datasets in the NCEA repository.** \n",
    "* This notebook reads each School Dataset file located at \\EducationDataNC\\ *schoolYear* \\School Datasets\\ as input data.\n",
    "* Different school years are processed by changing the *schoolYear* parameter.\n",
    "* Different input / output files are processed / created by changing the *inputFileName* paramter in the cell below.  \n",
    "* While a single program is used to create all the _ML datasets, one program copy per dataset is maintained in the repositiory so the dataset specific tranformation reports may be reviewed. \n",
    "\n",
    "**Datasets ending in ML are preprocessed for Machine Learning and go through the following transformations: **\n",
    "1. Missing student body racial compositions are imputed using district averages.\n",
    "2. Columns that have the same value in every single row are deleted.\n",
    "3. Columns that have a unique value in every single row (all values are different) are deleted.\n",
    "4. Empty columns (all values are NA or NULL) are deleted.\n",
    "5. Numeric columns with more than the percentage of missing values specified by the *missingThreshold* parameter.\n",
    "6. Remaining numeric, non-race columns with missing values are imputed / populated with 0.  In many cases, schools are not reporting values when they are zero. However, mean imputation or some other more sophisticated strategy might be considered here.\n",
    "7. Categorical / text based columns with > *uniqueThreshold* unique values are deleted.\n",
    "8. All remaining categorical / text based columns are one-hot encoded.  In categorical columns, one-hot encoding creates one new boolean / binary field per unique value in the target column, converting all categorical columns to a numeric data type. \n",
    "9. Duplicated or highly similar columns with > 95% correlation are delelted.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********Start: Beginning Column and Row Counts********************************************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 530 entries, 0 to 529\n",
      "Columns: 2304 entries, vphone_ad to Attrtion_Reason_OtherReasonsPerct\n",
      "dtypes: float64(2253), int64(3), object(48)\n",
      "memory usage: 9.3+ MB\n",
      "\n",
      "*********After: Selecting Only Public School Campuses**********************************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 530 entries, 0 to 529\n",
      "Columns: 2304 entries, vphone_ad to Attrtion_Reason_OtherReasonsPerct\n",
      "dtypes: float64(2253), int64(3), object(48)\n",
      "memory usage: 9.3+ MB\n"
     ]
    }
   ],
   "source": [
    "#import required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "\n",
    "#**********************************************************************************\n",
    "# Set the following variables before running this code!!!\n",
    "#**********************************************************************************\n",
    "#All raw data files are processed for the year below\n",
    "schoolYear = 2017\n",
    "\n",
    "#Location where copies of the raw data files will be read in from csv files.\n",
    "dataDir = '../../../' + str(schoolYear) + '/School Datasets/'\n",
    "\n",
    "#Name of the file to be processed\n",
    "#inputFileName = 'PublicSchools2017'\n",
    "#inputFileName = 'PublicHighSchools2017'\n",
    "inputFileName = 'PublicMiddleSchools2017'\n",
    "#inputFileName = 'PublicElementarySchools2017'\n",
    "\n",
    "#Input file being transformed for machine learning \n",
    "inputFile = dataDir + inputFileName + '.csv'\n",
    "\n",
    "#Location where the new school datasets will be created.\n",
    "outputDir = '../../../' + str(schoolYear) + '/Machine Learning Datasets/'\n",
    "\n",
    "#Missing Data Threshold (Per Column)\n",
    "missingThreshold = 0.60\n",
    "\n",
    "#Unique Value Threshold (Per Column)\n",
    "#Delete Columns >  uniqueThreshold unique values prior to one-hot encoding. \n",
    "#(each unique value becomes a new column during one-hot encoding)\n",
    "uniqueThreshold = 25\n",
    "\n",
    "#Read in the School Data File\n",
    "schoolData = pd.read_csv(inputFile, low_memory=False, dtype={'unit_code': object})\n",
    "print('*********Start: Beginning Column and Row Counts********************************************')\n",
    "schoolData.info(verbose=False)\n",
    "\n",
    "#Select only public schools as charter schools are missing data for many columns.\n",
    "schoolData = schoolData[(schoolData['type_cd'] == 'P') & (schoolData['student_num'] > 0)]\n",
    "\n",
    "print('\\r\\n*********After: Selecting Only Public School Campuses**********************************')\n",
    "schoolData.info(verbose=False)\n",
    "\n",
    "#Save primary key\n",
    "unit_code = schoolData['unit_code']\n",
    "#Convert zip code to string\n",
    "schoolData['szip_ad'] = schoolData['szip_ad'].astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Consolidated Dataset for Machine Learning\n",
    "**Below we perform operations on the entire dataset to remove columns and update row values that could cause problems during machine learning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Body Racial Composition Features \n",
    "**Impute / update missing Student Body Racial Composition Fields using mean imputation.**\n",
    "* When there are no racial composition percentages for a particular school campus / unit_code, fill in the missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********After: Updating Missing Racial Compostion Values****************************\n",
      "Rows Updated / Imputed:  0\n",
      "\r\n",
      "Total Rows Missing Racial Compositions By District Name\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], Name: District Name, dtype: int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get Student Body Racial Composition Fields\n",
    "raceCompositionFields = schoolData.filter(regex='Indian|Asian|Hispanic|Black|White|PacificIsland|TwoOrMore|Minority')\\\n",
    "                                  .filter(regex='Pct').columns\n",
    "    \n",
    "rowsBefore = schoolData[raceCompositionFields].isnull().T.any().T.sum()\n",
    "\n",
    "#Update missing race values with the district average when avaiable (No district averages for charter schools) \n",
    "schoolData[raceCompositionFields] = schoolData.groupby('District Name')[raceCompositionFields]\\\n",
    "                                              .transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "    #Review dataset contents after Racial Composition Imputation\n",
    "print('*********After: Updating Missing Racial Compostion Values****************************')   \n",
    "rowsAfter = schoolData[raceCompositionFields].isnull().T.any().T.sum()\n",
    "rowsUpdated = rowsBefore - rowsAfter\n",
    "print ('Rows Updated / Imputed: ', rowsUpdated )\n",
    "print('\\r\\nTotal Rows Missing Racial Compositions By District Name') \n",
    "schoolData['District Name'][schoolData[raceCompositionFields].isnull().T.any().T].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Columns with Problematic Data\n",
    "**Here we remove entire columns that could cause problems during machine learning.  The following operations are performed:**\n",
    "* Remove any columns that have the same value in every single row.\n",
    "* Remove any columns that have a unique value in every single row (all values are different).\n",
    "* Remove empty columns (all values are NA or NULL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********After: Removing columns with the same value in every row.*******************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 530 entries, 0 to 529\n",
      "Columns: 2099 entries, vphone_ad to Attrtion_Reason_OtherReasonsPerct\n",
      "dtypes: float64(2054), object(45)\n",
      "memory usage: 8.5+ MB\n",
      "\r\n",
      "Columns Deleted:  205\n"
     ]
    }
   ],
   "source": [
    "#Remove any fields that have the same value in all rows\n",
    "UniqueValueCounts = schoolData.nunique(dropna=False)\n",
    "SingleValueCols = UniqueValueCounts[UniqueValueCounts == 1].index\n",
    "schoolData = schoolData.drop(SingleValueCols, axis=1)\n",
    "\n",
    "#Review dataset contents after drops\n",
    "print('*********After: Removing columns with the same value in every row.*******************')\n",
    "schoolData.info(verbose=False)\n",
    "print ('\\r\\nColumns Deleted: ', len(SingleValueCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********After: Removing columns with unique values in every row.*******************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 530 entries, 0 to 529\n",
      "Columns: 2099 entries, vphone_ad to Attrtion_Reason_OtherReasonsPerct\n",
      "dtypes: float64(2054), object(45)\n",
      "memory usage: 8.5+ MB\n",
      "\r\n",
      "Columns Deleted:  0\n"
     ]
    }
   ],
   "source": [
    "#Remove any fields that have unique values in every row\n",
    "schoolDataRecordCt = schoolData.shape[0]\n",
    "UniqueValueCounts = schoolData.apply(pd.Series.nunique)\n",
    "AllUniqueValueCols = UniqueValueCounts[UniqueValueCounts == schoolDataRecordCt].index\n",
    "schoolData = schoolData.drop(AllUniqueValueCols, axis=1)\n",
    "\n",
    "#Review dataset contents after drops\n",
    "print('*********After: Removing columns with unique values in every row.*******************')\n",
    "schoolData.info(verbose=False)\n",
    "print ('\\r\\nColumns Deleted: ', len(AllUniqueValueCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-1e19ad2c6e47>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-1e19ad2c6e47>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    print '\\r\\nColumns Deleted: ', len(NullValueCols)\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Remove any empty fields (null values in every row)\n",
    "schoolDataRecordCt = schoolData.shape[0]\n",
    "NullValueCounts = schoolData.isnull().sum()\n",
    "NullValueCols = NullValueCounts[NullValueCounts == schoolDataRecordCt].index\n",
    "schoolData = schoolData.drop(NullValueCols, axis=1)\n",
    "\n",
    "#Review dataset contents after empty field drops\n",
    "print('*********After: Removing columns with null / blank values in every row.*************')\n",
    "schoolData.info(verbose=False)\n",
    "print '\\r\\nColumns Deleted: ', len(NullValueCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Other Missing Values Types\n",
    "* Here we eliminate any numeric columns with more than the percentage of missing values specified by the *missingThreshold* parameter.\n",
    "* All remaining non-race, numeric column missing values are populated with 0.\n",
    "* In many cases, it seems that schools are not simply not reporting values when they are zero. However, mean imputation or some other strategy might be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolate continuous and categorical data types\n",
    "#These are indexers into the schoolData dataframe and may be used similar to the schoolData dataframe \n",
    "sD_boolean = schoolData.loc[:, (schoolData.dtypes == bool) ]\n",
    "sD_nominal = schoolData.loc[:, (schoolData.dtypes == object)]\n",
    "sD_continuous = schoolData.loc[:, (schoolData.dtypes != bool) & (schoolData.dtypes != object)]\n",
    "print (\"Boolean Columns: \", sD_boolean.shape[1])\n",
    "print (\"Nominal Columns: \", sD_nominal.shape[1])\n",
    "print (\"Continuous Columns: \", sD_continuous.shape[1])\n",
    "print (\"Columns Accounted for: \", sD_nominal.shape[1] + sD_continuous.shape[1] + sD_boolean.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminate continuous columns with more than missingThreshold percentage of missing values\n",
    "schoolDataRecordCt = sD_continuous.shape[0]\n",
    "missingValueLimit = schoolDataRecordCt * missingThreshold\n",
    "NullValueCounts = sD_continuous.isnull().sum()\n",
    "NullValueCols = NullValueCounts[NullValueCounts >= missingValueLimit].index\n",
    "schoolData = schoolData.drop(NullValueCols, axis=1)\n",
    "\n",
    "#Review dataset contents after empty field drops\n",
    "print('*********After: Removing columns with >= missingThreshold % of missing values******')\n",
    "schoolData.info(verbose=False)\n",
    "print ('\\r\\nColumns Deleted: ', len(NullValueCols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding of Categorical Variables\n",
    "**All categorical / string variables are converted to numberic variables via one hot encoding.  Each unique row value will become a new binary / numeric column in the dataset.**\n",
    "* All remaining categorical columns are one-hot encoded.  \n",
    "* In categorical columns, one-hot encoding creates one new boolean / binary field per unique value in the target column, converting all categorical columns to a numeric data type. \n",
    "* Prior to one-hot encoding, columns with > *uniqueThreshold* unique values are deleted.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete categorical columns with > 25 unique values (Each unique value becomes a column during one-hot encoding)\n",
    "oneHotUniqueValueCounts = schoolData[sD_nominal.columns].apply(lambda x: x.nunique())\n",
    "oneHotUniqueValueCols = oneHotUniqueValueCounts[oneHotUniqueValueCounts >= uniqueThreshold].index\n",
    "schoolData.drop(oneHotUniqueValueCols, axis=1, inplace=True) \n",
    "\n",
    "#Review dataset contents one hot high unique value drops\n",
    "print('*********After: Removing columns with >= uniqueThreshold unique values***********')\n",
    "schoolData.info(verbose=False)\n",
    "print ('\\r\\nColumns Deleted: ', len(oneHotUniqueValueCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolate remaining categorical variables\n",
    "begColumnCt = len(schoolData.columns)\n",
    "sD_nominal = schoolData.loc[:, (schoolData.dtypes == object)]\n",
    "\n",
    "#one hot encode categorical variables\n",
    "schoolData = pd.get_dummies(data=schoolData, \n",
    "                       columns=sD_nominal.columns, drop_first=True)\n",
    "\n",
    "#Determine change in column count\n",
    "endColumnCt = len(schoolData.columns)\n",
    "columnsAdded = endColumnCt - begColumnCt\n",
    "\n",
    "#Review dataset contents one hot high unique value drops\n",
    "print( 'Columns To One-Hot Encode: ', len(sD_nominal.columns))\n",
    "print('\\r\\n*********After: Adding New Columns Via One-Hot Encoding*************************')\n",
    "schoolData.info(verbose=False)\n",
    "print ('\\r\\nNew Columns Created Via One-Hot Encoding: ', columnsAdded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute any Remaining Missing Values as Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out all the missing value rows\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "print('\\r\\n*********The Remaining Missing Values Below will be set to Zero!*************************')\n",
    "\n",
    "#Check for Missing values \n",
    "missing_values = schoolData.isnull().sum().reset_index()\n",
    "missing_values.columns = ['Variable Name', 'Number Missing Values']\n",
    "missing_values = missing_values[missing_values['Number Missing Values'] > 0] \n",
    "missing_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all remaining NaN with 0\n",
    "schoolData = schoolData.fillna(0)\n",
    "\n",
    "#Check for Missing values after final imputation \n",
    "missing_values = schoolData.isnull().sum().reset_index()\n",
    "missing_values.columns = ['Variable Name', 'Number Missing Values']\n",
    "missing_values = missing_values[missing_values['Number Missing Values'] > 0] \n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and Remove Highly Correlated Features\n",
    "**Find and remove any columns / features that are > 95% correlated**\n",
    "* https://stackoverflow.com/questions/39409866/correlation-heatmap\n",
    "* https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
    "* https://codeyarns.com/2015/04/20/how-to-change-font-size-in-seaborn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the correlation matrix\n",
    "corr_matrix  = schoolData.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all of the correlation values > 95%\n",
    "x = np.where(upper > 0.95)\n",
    "\n",
    "#Display all field combinations with > 95% correlation\n",
    "cf = pd.DataFrame()\n",
    "cf['Field1'] = upper.columns[x[1]]\n",
    "cf['Field2'] = upper.index[x[0]]\n",
    "\n",
    "#Get the correlation values for every field combination. (There must be a more pythonic way to do this!)\n",
    "corr = [0] * len(cf)\n",
    "for i in range(0, len(cf)):\n",
    "    corr[i] =  upper[cf['Field1'][i]][cf['Field2'][i]] \n",
    "    \n",
    "cf['Correlation'] = corr\n",
    "\n",
    "print ('There are ', str(len(cf['Field1'])), ' field correlations > 95%.')\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Dropping the following ', str(len(to_drop)), ' highly correlated fields.')\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check columns before drop \n",
    "print('\\r\\n*********Before: Dropping Highly Correlated Fields*************************************')\n",
    "schoolData.info(verbose=False)\n",
    "\n",
    "# Drop the highly correlated features from our training data \n",
    "schoolData = schoolData.drop(to_drop, axis=1)\n",
    "\n",
    "#Check columns after drop \n",
    "print('\\r\\n*********After: Dropping Highly Correlated Fields**************************************')\n",
    "schoolData.info(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restore the unit_code before saving\n",
    "schoolData['unit_code'] = unit_code\n",
    "#Save the final dataset to a .csv file\n",
    "schoolData.to_csv(outputDir + inputFileName + '_ML.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*********FINAL DATASET DETAILS*********************************************************\\r\\n')\n",
    "schoolData.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "print('Sklearn Version: ' + sklearn.__version__)\n",
    "print('Pandas Version: ' + pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Output File Location:\\r\\n\\r\\n' + outputDir + inputFileName + '_ML.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
