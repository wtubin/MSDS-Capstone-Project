{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Teacher Attrition Rates for Public Schools\n",
    "**This notebook explores predicting teacher attrition rates for public schools in North Carolina.** \n",
    "* Public school racial compositions are also considered when making predictions in this notebook.\n",
    "* The North Carolina Educational Attainment Data Repository for Machine Learning is located on Github at: https://github.com/wtubin/TeacherRetentionofNC\n",
    "\n",
    "**For documentation on various Generalized Linear Models in Sklearn see:**\n",
    "* http://scikit-learn.org/stable/modules/linear_model.html\n",
    "* https://stackoverflow.com/questions/33845539/modelling-probabilities-in-a-regularized-logistic-regression-model-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required Libraries\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV    \n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "from sklearn.model_selection import cross_validate\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost\n",
    "import math\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 'path' variable may also be a URL pointing to the proper raw file path on github: \n",
    "\n",
    "ML2017 = '../Machine Learning Datasets/PublicSchools2017_ML.csv'\n",
    "ML2016 = '../../2016/Machine Learning Datasets/PublicSchools2016_ML.csv'\n",
    "ML2015 =  '../../2015/Machine Learning Datasets/PublicSchools2015_ML.csv'\n",
    "ML2014 =  '../../2014/Machine Learning Datasets/PublicSchools2014_ML.csv'\n",
    "\n",
    "schData2017=pd.read_csv(ML2017, low_memory=False)\n",
    "print('*********************************2017 ML Data*************************************')\n",
    "schData2017.info()\n",
    "schData2016=pd.read_csv(ML2016, low_memory=False)\n",
    "print('*********************************2016 ML Data*************************************')\n",
    "schData2016.info()\n",
    "\n",
    "schData2015=pd.read_csv(ML2015, low_memory=False)\n",
    "print('*********************************2015 ML Data*************************************')\n",
    "schData2015.info()\n",
    "\n",
    "schData2014=pd.read_csv(ML2014, low_memory=False)\n",
    "print('*********************************2014 ML Data*************************************')\n",
    "schData2014.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schData2014[\"Year\"]=2014\n",
    "schData2015[\"Year\"]=2015\n",
    "schData2016[\"Year\"]=2016\n",
    "schData2017[\"Year\"]=2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data set\n",
    "schData = pd.concat([schData2017,schData2016,schData2015,schData2014], axis=0, ignore_index=True, sort = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Data Threshold (Per Column)\n",
    "missingThreshold = 0.60\n",
    "\n",
    "#Unique Value Threshold (Per Column)\n",
    "#Delete Columns >  uniqueThreshold unique values prior to one-hot encoding. \n",
    "#(each unique value becomes a new column during one-hot encoding)\n",
    "uniqueThreshold = 25\n",
    "\n",
    "\n",
    "#Eliminate continuous columns with more than missingThreshold percentage of missing values\n",
    "schoolDataRecordCt = schData.shape[0]\n",
    "missingValueLimit = schoolDataRecordCt * missingThreshold\n",
    "NullValueCounts = schData.isnull().sum()\n",
    "NullValueCols = NullValueCounts[NullValueCounts >= missingValueLimit].index\n",
    "schData = schData.drop(NullValueCols, axis=1)\n",
    "\n",
    "#Review dataset contents after empty field drops\n",
    "print('*********After: Removing columns with >= missingThreshold % of missing values******')\n",
    "schData.info(verbose=False)\n",
    "print ('\\r\\nColumns Deleted: ', len(NullValueCols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out all the missing value rows\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "print('\\r\\n*********The Remaining Missing Values Below will be set to Zero!*************************')\n",
    "\n",
    "#Check for Missing values \n",
    "missing_values = schData.isnull().sum().reset_index()\n",
    "missing_values.columns = ['Variable Name', 'Number Missing Values']\n",
    "missing_values = missing_values[missing_values['Number Missing Values'] > 0] \n",
    "missing_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all remaining NaN with 0\n",
    "schData = schData.fillna(0)\n",
    "\n",
    "#Check for Missing values after final imputation \n",
    "missing_values = schData.isnull().sum().reset_index()\n",
    "missing_values.columns = ['Variable Name', 'Number Missing Values']\n",
    "missing_values = missing_values[missing_values['Number Missing Values'] > 0] \n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we are not remove the 0 1 year percentage for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeroAttrition = schData[schData['_1yr_tchr_trnovr_pct']==0.0].index\n",
    "# schData = schData.drop(zeroAttrition,axis=0)\n",
    "# print('*********After: Removing rows with first year turn over percentage =0.00******')\n",
    "# schData.info(verbose=False)\n",
    "# print ('\\r\\nRows Deleted: ', len(zeroAttrition))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,10))\n",
    "_1yr_tchr_trnovr_pct = schData[\"_1yr_tchr_trnovr_pct\"]\n",
    "ax = sns.distplot(_1yr_tchr_trnovr_pct);\n",
    "plt.title(\"Figure 1. North Carolina Public School First Year Teacher Turnover Percentage (2014-2017)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schData['lea_salary'] = schData['lea_total_expense_num']*schData['lea_salary_expense_pct']\n",
    "\n",
    "plt.figure(figsize=(13,10))\n",
    "lea_salary_expense_pct=schData.lea_salary_expense_pct\n",
    "ax = sns.distplot(schData['lea_salary']);\n",
    "plt.title(\"Figure 2. North Carolina Public School Lea Salary Expense (2014-2017)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# responseible variabl distribution\n",
    "schData['_1yr_tchr_trnovr_pct'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"Year\", y=\"_1yr_tchr_trnovr_pct\", data=schData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we may want to check the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sns.lmplot(x=\"Crime_Rate_Murder_Rate\", y=\"_1yr_tchr_trnovr_pct\", hue = \"Year\", data=schData,height=8.27, aspect=11.7/8.27)\n",
    "sns.despine()\n",
    "\n",
    "plt.title(\"Figure 3. North Carolina Public School One Year Teacher Attrition Rate vs. LEA Salary Expenditure (2014-2017)\")\n",
    "plt.ylabel('1st Year Teacher Attrition Rate')\n",
    "plt.xlabel('Crime Rate Murder Rate')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"lea_total_expense_num\", y=\"_1yr_tchr_trnovr_pct\", hue=\"Year\", data=schData,height=8.27, aspect=11.7/8.27)\n",
    "sns.despine()\n",
    "\n",
    "plt.title(\"Figure 4. North Coralina Public School One Year Teacher Attrition Rate vs. LEA Total School Expenditure (2014-2017)\")\n",
    "plt.ylabel('1st Year Attrition Rate')\n",
    "plt.xlabel('LEA Total School Expenditure')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"lea_total_expense_num\", y=\"_1yr_tchr_trnovr_pct\", hue=\"Year\", data=schData,height=8.27, aspect=11.7/8.27)\n",
    "sns.despine()\n",
    "\n",
    "plt.title(\"Figure 4. North Coralina Public School One Year Teacher Attrition Rate vs. LEA Total School Expenditure (2014-2017)\")\n",
    "plt.ylabel('1st Year Attrition Rate')\n",
    "plt.xlabel('LEA Total School Expenditure')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#schData.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "* set max features we want to select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually remove columns\n",
    "\n",
    "* there columns is not what we are interested in, so we remove them manaually from data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually remove the columns\n",
    "\n",
    "ManualDropFeatures = ['st_1yr_tchr_trnovr_pct','lea_1yr_tchr_trnovr_pct','PacificIslandPct','PacificIslandMalePct','PacificIslandFemalePct','HispanicPct'\n",
    "                      ,'BlackPct','AsianPct','IndianPct','WhitePct'\n",
    "                      ,'lea_supplies_expense_pct'\n",
    "                      ,'lea_services_expense_pct','lea_salary_expense_pct','lea_total_expense_num','lea_benefits_expense_pct'                    \n",
    "                     ]\n",
    "\n",
    "schData = schData.drop(ManualDropFeatures,axis=1)\n",
    "print('*********After: Removing fetures we are not interested in ******')\n",
    "schData.info(verbose=False)\n",
    "print ('\\r\\nFeatures Deleted: ', len(ManualDropFeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threthold for the feature selection\n",
    "max_features =350\n",
    "# backup the data set\n",
    "schData_bak = schData\n",
    "#schData = schData_bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X and y\n",
    "y= schData['_1yr_tchr_trnovr_pct']\n",
    "X = schData.drop(['_1yr_tchr_trnovr_pct'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection \n",
    "X_fs = X\n",
    "y_fs = y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson Correlation\n",
    "feature_name = X_fs.columns.tolist()\n",
    "def cor_selector(X, y):\n",
    "    cor_list = []\n",
    "    # calculate the correlation with y for each feature\n",
    "    for i in X.columns.tolist():\n",
    "        cor = np.corrcoef(X[i], y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "        # replace NaN with 0\n",
    "        cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "        # feature name\n",
    "        cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-max_features:]].columns.tolist()\n",
    "        # feature selection? 0 for not select, 1 for select\n",
    "        cor_support = [True if i in cor_feature else False for i in feature_name]\n",
    "    return cor_support, cor_feature\n",
    "cor_support, cor_feature = cor_selector(X_fs, y_fs)\n",
    "print(str(len(cor_feature)), 'selected features')\n",
    "print(pd.DataFrame( cor_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Reggesion RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "rfe_selector = RFE(estimator=LinearRegression(), n_features_to_select=max_features, step=10, verbose=5)\n",
    "rfe_selector.fit(X_fs, y_fs)\n",
    "rfe_support = rfe_selector.get_support()\n",
    "rfe_feature = X_fs.loc[:,rfe_support].columns.tolist()\n",
    "print(str(len(rfe_feature)), 'selected features')\n",
    "print(pd.DataFrame(rfe_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "Linear = LinearRegression()\n",
    "\n",
    "Linear_selector = SelectFromModel(Linear, threshold='1.25*median', max_features=max_features)\n",
    "Linear_selector.fit(X_fs, y_fs)\n",
    "Linear_support = Linear_selector.get_support()\n",
    "Linear_feature = X_fs.loc[:,Linear_support].columns.tolist()\n",
    "print(str(len(Linear_feature)), 'selected features')\n",
    "print(pd.DataFrame(Linear_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "Lasso = LassoCV(cv = 5)\n",
    "\n",
    "LassoCV_selector = SelectFromModel(Lasso, threshold='1.25*median', max_features=max_features)\n",
    "LassoCV_selector.fit(X_fs, y_fs)\n",
    "LassoCV_support = LassoCV_selector.get_support()\n",
    "LassoCV_feature = X_fs.loc[:,LassoCV_support].columns.tolist()\n",
    "print(str(len(LassoCV_feature)), 'selected features')\n",
    "print(pd.DataFrame(LassoCV_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "reg = Ridge()\n",
    "\n",
    "reg_selector = SelectFromModel(reg, threshold='1.25*median', max_features=max_features)\n",
    "reg_selector.fit(X_fs, y_fs)\n",
    "reg_support = reg_selector.get_support()\n",
    "reg_feature = X_fs.loc[:,reg_support].columns.tolist()\n",
    "print(str(len(reg_feature)), 'selected features')\n",
    "print(pd.DataFrame(reg_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "ELN = ElasticNet()\n",
    "\n",
    "ELN_selector = SelectFromModel(ELN, threshold='1.25*median', max_features=max_features)\n",
    "ELN_selector.fit(X_fs, y_fs)\n",
    "ELN_support = ELN_selector.get_support()\n",
    "ELN_feature = X_fs.loc[:,ELN_support].columns.tolist()\n",
    "print(str(len(ELN_feature)), 'selected features')\n",
    "print(pd.DataFrame(ELN_feature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary\n",
    "pd.set_option('display.max_rows', None)\n",
    "# put all selection together\n",
    "feature_selection_df = pd.DataFrame({'Feature':feature_name\n",
    "                                     , 'Pearson':cor_support\n",
    "                                     , 'Linear Regression':Linear_support\n",
    "                                     , 'RFE':rfe_support\n",
    "                                     , 'Lasso':LassoCV_support\n",
    "                                     ,'Ridge':reg_support\n",
    "                                     , 'Elastic Net':ELN_support\n",
    "                                     \n",
    "                                    })\n",
    "# count the selected times for each feature\n",
    "feature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n",
    "# display the top 100\n",
    "feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] ,\n",
    "ascending=False)\n",
    "feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
    "feature_selection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export feature selection to csv file\n",
    "feature_selection_df.to_csv('Feature_Selection_Results.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_remove = feature_selection_df[feature_selection_df['Total']<3]['Feature']\n",
    "X = X.drop(feature_to_remove, axis=1)\n",
    "\n",
    "#feature_to_remove\n",
    "print ('\\r\\nFeatures Deleted: ', len(feature_to_remove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "\n",
    "saved_cols = X.columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "#Save as data frames\n",
    "X = pd.DataFrame(X)\n",
    "X.columns = saved_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# df2_vif = pd.DataFrame()\n",
    "# df2_vif[\"VIF Factor\"] = [vif(X.values, i) for i in range(X.shape[1])]\n",
    "# df2_vif[\"ABSVIFFactor\"] = abs(df2_vif[\"VIF Factor\"])\n",
    "# df2_vif[\"features\"] = X.columns\n",
    "# df2_vif[\"indexes\"] = range(0, len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2_vif = df2_vif.sort_values('ABSVIFFactor', ascending=True)\n",
    "# df2_vif.to_csv('VIF_Result_ML.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "#     test_size=0.3,random_state =1 )\n",
    "\n",
    "# #print(\"RFE CV Linear Regression 1st Pass\")\n",
    "# rfecvEstimator = LinearRegression()\n",
    "\n",
    "# parameters = { 'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\n",
    "\n",
    "# #Create a grid search object  \n",
    "\n",
    "# grid = GridSearchCV(estimator=rfecvEstimator\n",
    "#                    #, n_jobs=8 # jobs to run in parallel\n",
    "#                    #, verbose=0 # low verbosity\n",
    "#                    , param_grid=parameters\n",
    "#                   )\n",
    "\n",
    "# #Perform hyperparameter search to find the best combination of parameters for our data using RFECV\n",
    "# grid.fit(X_train, y_train)\n",
    "# print(\"r2 / variance : \", grid.best_score_)\n",
    "# print(\"Residual sum of squares: %.2f\"\n",
    "#               % np.mean((grid.predict(X_test) - y_test) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create Cross Validation Object with 10 folds with 80/20 train - test split\n",
    "cv = ShuffleSplit(n_splits = 10, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use mean absolute error (MAE) to score the regression models created \n",
    "#(the scale of MAE is identical to the response variable)\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error, r2_score \n",
    "#from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error, r2_scorer \n",
    "\n",
    "#Function for Root mean squared error\n",
    "#https://stackoverflow.com/questions/17197492/root-mean-square-error-in-python\n",
    "def rmse(y_actual, y_predicted):\n",
    "    return np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "\n",
    "#Function for Mean Absolute Percentage Error (MAPE) - Untested\n",
    "#Adapted from - https://stackoverflow.com/questions/42250958/how-to-optimize-mape-code-in-python\n",
    "# revised from - https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn\n",
    "# still not working\n",
    "def mape(y_actual, y_predicted): \n",
    "    mask = y_actual != 0           \n",
    "    return (np.fabs(y_actual - y_predicted)/y_actual)[mask].mean() * 100\n",
    "# Credit: https://alex.miller.im/posts/linear-model-custom-loss-function-regularization-python/\n",
    "def mean_absolute_percentage_error(y_true, y_pred, sample_weights=None):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    \n",
    "    if np.any(y_true==0):\n",
    "        print(\"Found zeroes in y_true. MAPE undefined. Removing from set...\")\n",
    "        idx = np.where(y_true==0)\n",
    "        y_true = np.delete(y_true, idx)\n",
    "        y_pred = np.delete(y_pred, idx)\n",
    "        if type(sample_weights) != type(None):\n",
    "            sample_weights = np.array(sample_weights)\n",
    "            sample_weights = np.delete(sample_weights, idx)\n",
    "        \n",
    "    if type(sample_weights) == type(None):\n",
    "        return(np.mean(np.abs((y_true - y_pred) / y_true)))\n",
    "    else:\n",
    "        sample_weights = np.array(sample_weights)\n",
    "        assert len(sample_weights) == len(y_true)\n",
    "        return(100/sum(sample_weights)*np.dot(\n",
    "                sample_weights, (np.abs((y_true - y_pred) / y_true))\n",
    "        ))\n",
    "    \n",
    "#Create scorers for rmse and mape functions\n",
    "mae_scorer = make_scorer(score_func=mean_absolute_error, greater_is_better=False)\n",
    "mse_scorer = make_scorer(score_func=mean_squared_error, greater_is_better=False)\n",
    "rmse_scorer = make_scorer(score_func=rmse, greater_is_better=False)\n",
    "#mape_scorer = make_scorer(score_func=mape, greater_is_better=False)\n",
    "r2_scorer = make_scorer(score_func=r2_score, greater_is_better=True) \n",
    "\n",
    "#Make scorer array to pass into cross_validate() function for producing mutiple scores for each cv fold.\n",
    "errorScoring = {'MAE':  mae_scorer, \n",
    "                'MSE':  mse_scorer, \n",
    "                'RMSE': rmse_scorer,\n",
    "                #'MAPE': mape_scorer,\n",
    "                'R2': r2_scorer       \n",
    "               } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def EvaluateRegressionEstimator(regEstimator, X, y, cv):\n",
    "    \n",
    "    scores = cross_validate(regEstimator, X, y, scoring=errorScoring, cv=cv, return_train_score=True)\n",
    "\n",
    "    #cross val score sign-flips the outputs of MAE\n",
    "    # https://github.com/scikit-learn/scikit-learn/issues/2439\n",
    "    scores['test_MAE'] = scores['test_MAE'] * -1\n",
    "    scores['test_MSE'] = scores['test_MSE'] * -1\n",
    "    #scores['test_MAPE'] = scores['test_MAPE'] * -1\n",
    "    scores['test_RMSE'] = scores['test_RMSE'] * -1\n",
    "\n",
    "    #print mean MAE for all folds \n",
    "    maeAvg = scores['test_MAE'].mean()\n",
    "    print_str = \"The average MAE for all cv folds is: \\t\\t\\t {maeAvg:.5}\"\n",
    "    print(print_str.format(maeAvg=maeAvg))\n",
    "\n",
    "     #print mean MAE for all folds \n",
    "    mseAvg = scores['test_MSE'].mean()\n",
    "    print_str = \"The average MSE for all cv folds is: \\t\\t\\t {mseAvg:.5}\"\n",
    "    print(print_str.format(mseAvg=mseAvg))\n",
    "    \n",
    "    #print mean test_MAPE for all folds\n",
    "#     scores['test_MAPE'] = scores['test_MAPE']\n",
    "#     mape_avg = scores['test_MAPE'].mean()\n",
    "#     print_str = \"The average MAE percentage (MAPE) for all cv folds is: \\t {mape_avg:.5}\"\n",
    "#     print(print_str.format(mape_avg=mape_avg))\n",
    "\n",
    "    #print mean MAE for all folds \n",
    "    RMSEavg = scores['test_RMSE'].mean()\n",
    "    print_str = \"The average RMSE for all cv folds is: \\t\\t\\t {RMSEavg:.5}\"\n",
    "    print(print_str.format(RMSEavg=RMSEavg))\n",
    "    print('*********************************************************')\n",
    "    \n",
    "    #print mean R-squared for all folds \n",
    "    R2avg = scores['test_R2'].mean()\n",
    "    print_str = \"The average R-Squared for all cv folds is: \\t\\t {R2avg:.5}\"\n",
    "    print(print_str.format(R2avg=R2avg))\n",
    "    print('*********************************************************')\n",
    "\n",
    "\n",
    "    print('Cross Validation Fold Mean Error Scores')\n",
    "    scoresResults = pd.DataFrame()\n",
    "    scoresResults['MAE'] = scores['test_MAE']\n",
    "    scoresResults['MSE'] = scores['test_MSE']\n",
    "   # scoresResults['MAPE'] = scores['test_MAPE']\n",
    "    scoresResults['RMSE'] = scores['test_RMSE']\n",
    "    scoresResults['R2'] = scores['test_R2']\n",
    "    return scoresResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "print(\"RFE CV Linear Regression 1st Pass\")\n",
    "rfecvEstimator = LinearRegression()\n",
    "\n",
    "parameters = { 'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]\n",
    "             }\n",
    "\n",
    "#Create a grid search object  \n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "rfecvGridSearch = GridSearchCV(estimator=rfecvEstimator\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                  , verbose=0 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   )\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data using RFECV\n",
    "rfecvGridSearch.fit(X, y)\n",
    "\n",
    "#Use the best parameters for our RFECV Linear Regression object\n",
    "rfecvLinearEst = rfecvGridSearch.best_estimator_\n",
    "\n",
    "#Recursive Feature Elimination\n",
    "rfecv = RFECV(estimator=rfecvLinearEst, step=1, cv=cv,  verbose=1)\n",
    "X_BestFeatures = rfecv.fit_transform(X, y)\n",
    "\n",
    "#Print RFECV Details\n",
    "print(\"Ranking\", rfecv.ranking_)\n",
    "print(\"Support\", rfecv.support_)\n",
    "print(\"Number of Features:\", rfecv.n_features_)\n",
    "\n",
    "print(\"Linear Regression Second Pass\")\n",
    "#create a pipeline to scale all of the data and perform logistic regression during each grid search step.\n",
    "pipe = make_pipeline(StandardScaler(), LinearRegression())\n",
    "\n",
    "#Define a range of hyper parameters for grid search\n",
    "parameters = { 'linearregression__fit_intercept':[True,False], 'linearregression__normalize':[True,False]\n",
    "              , 'linearregression__copy_X':[True, False]\n",
    "              , 'linearregression__n_jobs':[None]\n",
    "             }\n",
    "\n",
    "#Perform the grid search using accuracy as a metric during cross validation.\n",
    "grid = GridSearchCV(estimator=pipe, param_grid=parameters, cv=cv)\n",
    "\n",
    "#Use the best features from recursive feature elimination during the grid search\n",
    "grid.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the parameterization of the best estimator\n",
    "regEstimator = grid.best_estimator_\n",
    "\n",
    "RFE_Results = EvaluateRegressionEstimator(grid.best_estimator_, X_BestFeatures, y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model's coefficient weights and feature names into a dataframe sorted by weights\n",
    "weights = regEstimator.named_steps['linearregression'].coef_.ravel()#[rfecv.get_support(indices=True)]\n",
    "feature_names = X.columns#.values[rfecv.get_support(indices=True)]\n",
    "#print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model's coefficient weights and feature names into a dataframe sorted by weights\n",
    "#weights = grid.best_estimator_.named_steps['linearregression'].coef_.ravel()\n",
    "#feature_names = X.columns#.values[rfecv.get_support(indices=True)]\n",
    "\n",
    "linreg_ft_imp_df = pd.DataFrame({'feature_names':feature_names, 'weights':weights,\n",
    "                                 'absolute_weights': np.abs(weights)})\n",
    "linreg_ft_imp_df.sort_values(by='absolute_weights', inplace=True, ascending=False )\n",
    "\n",
    "#Print out all the missing value rows\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "#linreg_ft_imp_df = linreg_ft_imp_df[linreg_ft_imp_df.feature_names != 'SchoolYear']\n",
    "\n",
    "model_features = len(linreg_ft_imp_df.index)\n",
    "print('Total Model Features: ' + str(model_features))\n",
    "\n",
    "#Print all of the features selected by the model \n",
    "linreg_ft_imp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the top 75 coefficients\n",
    "wt_plt_df = linreg_ft_imp_df.head(50)\n",
    "\n",
    "model_features = len(wt_plt_df.index)\n",
    "print('Total Model Features: ' + str(model_features))\n",
    "\n",
    "# Examine categorical variables of interest  \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Plot the model's feature importances\n",
    "# REFERENCE:  Eric Larson, https://github.com/eclarson/DataMiningNotebooks\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "weights = pd.Series(wt_plt_df['weights'].values,index=wt_plt_df['feature_names'])\n",
    "ax = weights.plot(kind='bar', figsize=(20,8))\n",
    "\n",
    "ax.set_title(\"Top Feature Correlations\")\n",
    "ax.set_ylabel(\"Coefficient Magnitude\\n(z-score)\")\n",
    "ax.set_xlabel(\"Feature Names\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export feature selection to csv file\n",
    "linreg_ft_imp_df.to_csv('RFE_Feature_Importance_Results.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"Year\", y=\"calendar_type_txt_Regular School, Traditional Calendar\", data=schData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Several Features not making sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "**Cross validation is performed using repeated holdout using ShuffleSplit()**\n",
    "* Ten folds are used\n",
    "* The split is: 90% training data and 10% test data\n",
    "* A random seed is set so the same random test and training splits are used each time cross validation is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide data into test and training splits\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.10, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state =1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Scorers for Evaluating Regression Models \n",
    "\n",
    "**All regression models created in this notebook are validated using the following metrics:**\n",
    "* Mean Absolute Error (MAE)\n",
    "* Root Mean Squared Error (RMSE) - https://stackoverflow.com/questions/17197492/root-mean-square-error-in-python\n",
    "* Mean Absolute Percentage Error (MAPE) - https://stackoverflow.com/questions/42250958/how-to-optimize-mape-code-in-python\n",
    "\n",
    "**For details on making scorers to return multiple mean error scores see:**\n",
    "* http://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html\n",
    "* https://github.com/scikit-learn/scikit-learn/pull/7388\n",
    "* https://github.com/drorata/multiscorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it may not make sense to use MAPE in our project based on below articles:\n",
    "# http://www.catchbull.com/catchblogs/why-mape-doesnt-work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model Evaluation\n",
    "** All regression models are evaluated using the regression model evaluation function below: ** \n",
    "* The following regression evaluation function uses the cross validation object and the custom scorers in the two cells above in combination with sklearn.model_selection's cross_validate function to perform cross validation for regression estimators.\n",
    "* The cross validation object above uses a random seed to ensure that all regression estimators are tested on the same randomly selected records for each cross validation fold.\n",
    "* Custom scorers are created using the three chosen mean error scores and passed into cross_validate(), so all three scores are calcualted using a single call to cross_validate().\n",
    "* All of this functionality is wrapped within the custom EvaluateRegressionEstimator() function below so multiple regression models may be tested using the same test / train cv data and evaluation scores producing a consistent output for each model without the need to re-write the same code over and over. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Regression Model\n",
    "\n",
    "**Linear Regression is used to create a baseline model.  Since linear regression may predict response variable values outside the range of the training data's response variable, we create a linear regression estimator with graduation rate predictions clipped 0% and 100%. For details see:**\n",
    "* http://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator \n",
    "* https://github.com/scikit-learn/scikit-learn/issues/6950\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "* https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/template.py\n",
    "* https://stackoverflow.com/questions/44234682/how-to-use-sklearn-when-target-variable-is-a-proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make new estimator compatible for use with GridSearchCV() and cross_validate()\n",
    "# -  Cap predict function for LinearRegression between 0 and 100\n",
    "# -  See: Roll your own estimator links above for details. \n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class CappedLinearRegression(LinearRegression):\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.clip(super(CappedLinearRegression, self).predict(X), 0, 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Grid Search\n",
    "** Here we perform a grid search testing 40 models to find the best parameters for our Linear Regression model based on Mean Absolute Error.  See more on parameter tuning with grid search here:**\n",
    "* http://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Linear Regression object and perform a grid search to find the best parameters\n",
    "linreg = CappedLinearRegression()\n",
    "parameters = {'normalize':(True,False), 'fit_intercept':(True,False),'normalize': (True, False),}\n",
    "\n",
    "#Create a grid search object using the  \n",
    "regGridSearch = GridSearchCV(estimator=linreg\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the parameterization of the best estimator\n",
    "regEstimator = regGridSearch.best_estimator_\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "CappedLinear_Results = EvaluateRegressionEstimator(regEstimator, X, y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "regEstimator.fit(X_train, y_train)\n",
    "yhat = regEstimator.predict(X_test)\n",
    "print(\"Yhat Max: \", yhat.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Support Vector Regression\n",
    "**This model uses Support Vector Machines for regression of continuous variables (SVR). Please see documentation here:\"**\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "* http://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Linear regression object and perform a grid search to find the best parameters\n",
    "from sklearn.svm import SVR\n",
    "reg = SVR()\n",
    "\n",
    "#Set up SVR parameters to test (WARNING: Creates 320 models!!!) \n",
    "costs = [0.001, 0.1]\n",
    "defGamma = 1 / X.shape[1]  #This is the default value for the gamma parameter\n",
    "gammas = [defGamma, 0.1]\n",
    "kernels = ['rbf','linear']\n",
    "parameters = {'C': costs, 'gamma' : gammas, 'kernel': kernels}\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=reg\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the best estimator parameters\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVR\n",
    "\n",
    "#Create a regression estimator with best parameters for cross validation\n",
    "# regEstimator = SVR(C=0.001, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
    "#                    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "SVR_Results=EvaluateRegressionEstimator(regEstimator, X, y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do we predict graduation rates greater than 100%?\n",
    "# regEstimator = SVR(C=0.001, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
    "#                    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "regEstimator.fit(X_train, y_train)\n",
    "yhat = regEstimator.predict(X_test)\n",
    "print(\"Yhat Max: \", yhat.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graduation Rate - Lasso Regression\n",
    "**This model uses Lasso regression (L1 Norm). Please see documentation here:\"**\n",
    "* **Caution!** - See documentation for fit_intercept, normalize, and copy_X. Lasso can over-write your X data!\n",
    "* Lasso may also perform scaling as well.  Please see docs!\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a regression object and perform a grid search to find the best parameters\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "reg = Lasso(fit_intercept=True, normalize=True,copy_X=True\n",
    "          , max_iter=10000, precompute=True, tol=0.0001, random_state=0)\n",
    "\n",
    "#Test parameters \n",
    "alpha = [0.001, 0.1, 1, 10, 20]\n",
    "selection = ['cyclic','random']\n",
    "warm_start = [True, False]\n",
    "parameters = {'alpha': alpha, 'selection': selection, 'warm_start': warm_start}\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "regGridSearch = GridSearchCV(estimator=reg\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the best estimator parameters\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a regression estimator with best parameters for cross validation\n",
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "Lasso_Results = EvaluateRegressionEstimator(regEstimator, X, y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do we predict graduation rates greater than 100%?\n",
    "# regEstimator = Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=10000,\n",
    "#    normalize=True, positive=False, precompute=True, random_state=0,\n",
    "#    selection='cyclic', tol=0.0001, warm_start=True)\n",
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "regEstimator.fit(X_train, y_train)\n",
    "yhat = regEstimator.predict(X_test)\n",
    "print(\"Yhat Max: \", yhat.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "**This model uses Ridge regression (L2 Norm). Please see documentation here:\"**\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a regression object and perform a grid search to find the best parameters\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "reg = Ridge(fit_intercept=True, normalize=True,copy_X=True\n",
    "          , max_iter=10000, tol=0.0001, random_state=0)\n",
    "\n",
    "#Test parameters \n",
    "alpha = [0.001, 0.1, 1, 5, 10, 20]\n",
    "solver = [ 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "parameters = {'alpha': alpha, 'solver': solver}\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=reg\n",
    "                   , n_jobs=10 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the best estimator parameters\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Create a regression estimator with best parameters for cross validation\n",
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "Ridge_Results = EvaluateRegressionEstimator(regEstimator, X, y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do we predict graduation rates greater than 100%?\n",
    "# regEstimator = Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=10000,\n",
    "#    normalize=True, random_state=0, solver='saga', tol=0.0001)\n",
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "regEstimator.fit(X_train, y_train)\n",
    "yhat = regEstimator.predict(X_test)\n",
    "print(\"Yhat Max: \", yhat.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net Regression\n",
    "**This model uses Elastic Net Regression (L1 and L2 Norm mixing). Please see documentation here:\"**\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a regression object and perform a grid search to find the best parameters\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elasticNet = ElasticNet(fit_intercept=True, normalize=True, precompute=True, copy_X=True\n",
    "          , max_iter=10000, tol=0.0001, random_state=0)\n",
    " \n",
    "#Test parameters\n",
    "l1_ratio = [0.001, 0.01, 0.1, 0.5, 0.75, 1]\n",
    "alpha = [0.001, 0.1, 1, 10]\n",
    "selection = ['cyclic','random']\n",
    "warm_start = [True, False]\n",
    "parameters = {'l1_ratio': l1_ratio, 'alpha': alpha, 'selection': selection, 'warm_start': warm_start}\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "elasticNetGridSearch = GridSearchCV(estimator=elasticNet\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "elasticNetGridSearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the best estimator parameters\n",
    "elasticNetGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a regression estimator with best parameters for cross validation\n",
    "elasticNetEstimator = elasticNetGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "elastic_Results = EvaluateRegressionEstimator(elasticNetEstimator, X, y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do we predict graduation rates greater than 100%?\n",
    "elasticNetEstimator =  elasticNetGridSearch.best_estimator_\n",
    "\n",
    "elasticNetEstimator.fit(X_train, y_train)\n",
    "yhat = elasticNetEstimator.predict(X_test)\n",
    "print(\"Yhat Max: \", yhat.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit: https://www.kaggle.com/mburakergenc/predictions-with-xgboost-and-linear-regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try XGboost algorithm to see if we can get better results\n",
    "xgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf, testdf = train_test_split(X_train, test_size = 0.3)\n",
    "xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = xgb.predict(X_test)\n",
    "print(explained_variance_score(predictions,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
