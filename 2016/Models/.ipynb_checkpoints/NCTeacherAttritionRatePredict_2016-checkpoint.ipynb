{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Teacher Attrition Rates for Public Schools\n",
    "**This notebook explores predicting teacher attrition rates for public schools in North Carolina.** \n",
    "* Public school data, county crime rate data and school income tax data are considered when making predictions in this notebook.\n",
    "* The North Carolina Educational Attainment Data Repository for Machine Learning is located on Github at: https://github.com/wtubin/TeacherRetentionofNC\n",
    "\n",
    "**For documentation on various Generalized Linear Models in Sklearn see:**\n",
    "* http://scikit-learn.org/stable/modules/linear_model.html\n",
    "* https://stackoverflow.com/questions/33845539/modelling-probabilities-in-a-regularized-logistic-regression-model-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost\n",
    "#!pip install pyshp\n",
    "#!pip install ml_metrics \n",
    "# need c++ 1.04 http://go.microsoft.com/fwlink/?LinkId=691126&fixForIE=.exe.\n",
    "#!pip install minepy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required Libraries\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns; #sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV    \n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "from yellowbrick.features import JointPlotVisualizer\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import explained_variance_score\n",
    "import xgboost\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "#warnings.simplefilter('ignore')\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************************2016 ML Data*************************************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2430 entries, 0 to 2429\n",
      "Columns: 385 entries, closed_ind to unit_code\n",
      "dtypes: float64(307), int64(78)\n",
      "memory usage: 7.1 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#ML2017 = '../2017/Machine Learning Datasets/PublicSchools2017_ML.csv'\n",
    "ML2016 = '../Machine Learning Datasets/PublicSchools2016_ML.csv'\n",
    "# ML2015 =  '../2015/Machine Learning Datasets/PublicSchools2015_ML.csv'\n",
    "# ML2014 =  '../2014/Machine Learning Datasets/PublicSchools2014_ML.csv'\n",
    "\n",
    "# schData2017=pd.read_csv(ML2017, low_memory=False)\n",
    "# print('*********************************2017 ML Data*************************************')\n",
    "# schData2017.info()\n",
    "schData2016=pd.read_csv(ML2016, low_memory=False)\n",
    "print('*********************************2016 ML Data*************************************')\n",
    "schData2016.info()\n",
    "\n",
    "# schData2015=pd.read_csv(ML2015, low_memory=False)\n",
    "# print('*********************************2015 ML Data*************************************')\n",
    "# schData2015.info()\n",
    "\n",
    "# schData2014=pd.read_csv(ML2014, low_memory=False)\n",
    "# print('*********************************2014 ML Data*************************************')\n",
    "# schData2014.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schData2014[\"Year\"]=2014\n",
    "# schData2015[\"Year\"]=2015\n",
    "schData2016[\"Year\"]=2016\n",
    "# schData2017[\"Year\"]=2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'schData2017' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-01693462c452>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Combine the data sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# schData = pd.concat([schData2017,schData2016,schData2015,schData2014], axis=0, ignore_index=True, sort = True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mschData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mschData2017\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'schData2017' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine the data sets\n",
    "# schData = pd.concat([schData2017,schData2016,schData2015,schData2014], axis=0, ignore_index=True, sort = True)\n",
    "schData = schData2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove missing data after merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Data Threshold (Per Column)\n",
    "missingThreshold = 0.60\n",
    "\n",
    "#Unique Value Threshold (Per Column)\n",
    "#Delete Columns >  uniqueThreshold unique values prior to one-hot encoding. \n",
    "#(each unique value becomes a new column during one-hot encoding)\n",
    "uniqueThreshold = 25\n",
    "\n",
    "\n",
    "#Eliminate continuous columns with more than missingThreshold percentage of missing values\n",
    "schoolDataRecordCt = schData.shape[0]\n",
    "missingValueLimit = schoolDataRecordCt * missingThreshold\n",
    "NullValueCounts = schData.isnull().sum()\n",
    "NullValueCols = NullValueCounts[NullValueCounts >= missingValueLimit].index\n",
    "schData = schData.drop(NullValueCols, axis=1)\n",
    "\n",
    "#Review dataset contents after empty field drops\n",
    "print('*********After: Removing columns with >= missingThreshold % of missing values******')\n",
    "schData.info(verbose=False)\n",
    "print ('\\r\\nColumns Deleted: ', len(NullValueCols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all remaining NaN with 0\n",
    "schData = schData.fillna(0)\n",
    "\n",
    "#Check for Missing values after final imputation \n",
    "missing_values = schData.isnull().sum().reset_index()\n",
    "missing_values.columns = ['Variable Name', 'Number Missing Values']\n",
    "missing_values = missing_values[missing_values['Number Missing Values'] > 0] \n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schData['_1yr_tchr_trnovr_pct'].describe()\n",
    "# 'Number of farm returns All',\n",
    "# 'Number of farm returns 100KLT200K'\n",
    "# 'Number of farm returns LT25K',\n",
    "# 'Number of farm returns 25KLT50K',\n",
    "# 'Number of farm returns 50KLT75K',\n",
    "# 'Number of farm returns 75KLT100K',\n",
    "# 'Number of farm returns GE200K'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we are not remove the 0 1 year percentage for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ncreportcards.ondemand.sas.com/src/school?school=840344&year=2017&lang=english\n",
    "# verified the 1 year teacher turn over rate from above website. it looks like these are 0\n",
    "zeroAttrition = schData[schData['_1yr_tchr_trnovr_pct']==0.0].index\n",
    "\n",
    "#schData = schData.drop(zeroAttrition,axis=0)\n",
    "schData['_1yr_tchr_trnovr_pct'][schData['_1yr_tchr_trnovr_pct']==0.0] = 0.001\n",
    "\n",
    "#print('*********After: Removing rows with first year turn over percentage =0.00******')\n",
    "print('*********After: change turn over percentage =0.01******')\n",
    "schData.info(verbose=False)\n",
    "print ('\\r\\nRows Deleted: ', len(zeroAttrition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " schData[schData['_1yr_tchr_trnovr_pct']==0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visulization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Year Teacher Turnover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"North Carolina 2017 One Year Teacher Attrition Rate by zip code.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crime Index Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"North Carolina 2017 Crame Index Rate by zip code.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,10))\n",
    "_1yr_tchr_trnovr_pct = schData[\"_1yr_tchr_trnovr_pct\"]\n",
    "ax = sns.distplot(_1yr_tchr_trnovr_pct);\n",
    "plt.title(\"Figure 1. North Carolina Public School First Year Teacher Turnover Percentage (2014-2017)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schData['lea_salary'] = schData['lea_total_expense_num']*schData['lea_salary_expense_pct']\n",
    "\n",
    "plt.figure(figsize=(13,10))\n",
    "lea_salary_expense_pct=schData.lea_salary_expense_pct\n",
    "ax = sns.distplot(schData['lea_salary']);\n",
    "plt.title(\"Figure 2. North Carolina Public School Lea Salary Expense (2014-2017)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# responseible variabl distribution\n",
    "schData['_1yr_tchr_trnovr_pct'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we may want to check the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to plot regression lines for two variables\n",
    "def PlotJoint(x,y,data):\n",
    "    sns.set(context='notebook', style='darkgrid', font_scale=1.25)\n",
    "    my_plot = sns.jointplot(x=x, y=y, data=data, kind=\"reg\");\n",
    "    my_plot.fig.set_figwidth(8)\n",
    "    my_plot.fig.set_figheight(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #sns.set_style(\"darkgrid\")\n",
    "data = schData\n",
    "sns.lmplot(x=\"lea_salary\", y=\"_1yr_tchr_trnovr_pct\", hue = \"Year\",  data=data,height=8.27, aspect=11.7/8.27)\n",
    "sns.despine()\n",
    "\n",
    "#plt.title(\"Figure 3. North Carolina Public School One Year Teacher Attrition Rate vs. LEA Salary Expenditure (2014-2017)\")\n",
    "plt.ylabel('1st Year Teacher Attrition Rate')\n",
    "plt.xlabel('Lea Salary')\n",
    "#plt.show()\n",
    "plt.savefig('Figure1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #sns.set_style(\"darkgrid\")\n",
    "data = schData\n",
    "sns.lmplot(x=\"Number of farm returns All\", y=\"_1yr_tchr_trnovr_pct\", hue=\"Year\",  data=data,height=8.27, aspect=11.7/8.27)\n",
    "sns.despine()\n",
    "\n",
    "#plt.title(\"Figure 3. North Carolina Public School One Year Teacher Attrition Rate vs. LEA Salary Expenditure (2014-2017)\")\n",
    "plt.ylabel('1st Year Teacher Attrition Rate')\n",
    "plt.xlabel('Number of farm returns All')\n",
    "#plt.show()\n",
    "plt.savefig('Figure1.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"lea_total_expense_num\", y=\"_1yr_tchr_trnovr_pct\", hue=\"Year\", data=schData,height=8.27, aspect=11.7/8.27)\n",
    "sns.despine()\n",
    "\n",
    "#plt.title(\"Figure 4. North Coralina Public School One Year Teacher Attrition Rate vs. LEA Total School Expenditure (2014-2017)\")\n",
    "plt.ylabel('1st Year Attrition Rate')\n",
    "plt.xlabel('Lea Total Expense Num')\n",
    "plt.savefig('Figure2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair plot\n",
    "teacherProf = schData[['tchyrs_0thru3_pct','tchyrs_11plus_pct','0-3 Years_LEA_Exp_Pct_Prin','4-10 Years_LEA_Exp_Pct_Prin','flicensed_teach_pct','Year']]\n",
    "sns.pairplot(teacherProf\n",
    "             ,vars = ['tchyrs_0thru3_pct','tchyrs_11plus_pct','0-3 Years_LEA_Exp_Pct_Prin','4-10 Years_LEA_Exp_Pct_Prin','flicensed_teach_pct']\n",
    "             , hue = 'Year', diag_kind = 'kde', plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'},height = 4)\n",
    "# Title \n",
    "plt.suptitle('Pair Plot of Teacher Professional Development', \n",
    "             size = 28);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create district level charts to compare against Avg Tot Income\n",
    "teachprof = ['tchyrs_0thru3_pct','tchyrs_11plus_pct','0-3 Years_LEA_Exp_Pct_Prin','4-10 Years_LEA_Exp_Pct_Prin'\n",
    "             ,'flicensed_teach_pct','Proficient_TCHR_Standard 1_Pct','lateral_teach_pct'\n",
    "            \n",
    "            ]\n",
    "\n",
    "for col in teachprof:\n",
    "    #print(col)\n",
    "    try:\n",
    "        PlotJoint(x='_1yr_tchr_trnovr_pct',y=col, data=schData)\n",
    "    except Exception:\n",
    "        print(\"Error\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(schData, x_vars=['Crime_Rate_Index_Rate','Crime_Rate_Arson_Rate','crime_per_c_num']\n",
    "             , y_vars='_1yr_tchr_trnovr_pct', size=7, aspect=0.7, kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "ax = sns.boxplot(x=\"Year\", y=\"_1yr_tchr_trnovr_pct\", data=schData)\n",
    "ax = sns.swarmplot(x=\"Year\", y=\"_1yr_tchr_trnovr_pct\", data=schData, color=\".25\")\n",
    "plt.ylabel('1st Year Teacher Attrition Rate')\n",
    "plt.xlabel('Year')\n",
    "plt.savefig('Figure3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "* set max features we want to select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually remove columns\n",
    "\n",
    "* there columns is not what we are interested in, so we remove them manaually from data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually remove the columns\n",
    "\n",
    "ManualDropFeatures = ['st_1yr_tchr_trnovr_pct','lea_1yr_tchr_trnovr_pct'\n",
    "                     ,'PacificIslandPct','PacificIslandMalePct','PacificIslandFemalePct','HispanicPct'\n",
    "                      ,'BlackPct','AsianPct','IndianPct','WhitePct','TwoOrMorePct','TwoOrMoreMalePct','TwoOrMoreFemalePct'\n",
    "\n",
    "#                       ,'lea_supplies_expense_pct'\n",
    "#                       ,'lea_services_expense_pct','lea_salary_expense_pct'\n",
    "                      \n",
    "                      \n",
    "                      ,'lea_total_expense_num'\n",
    "                      #,'lea_benefits_expense_pct'\n",
    "                      ,'lea_state_perpupil_num'\n",
    "                      ,'lea_local_perpupil_num'\n",
    "                      ,'lea_federal_perpupil_num'\n",
    "                      \n",
    "                      ,'st_avg_student_num'\n",
    "                     ,'st_expelled_per_c_num'\n",
    "                     ,'st_crime_per_c_num'\n",
    "                     ,'st_avg_daily_attend_pct'\n",
    "                       ,'st_tchyrs_11plus_pct'\n",
    "                     ,'st_emer_prov_teach_pct'\n",
    "                     ,'st_long_susp_per_c_num'\n",
    "                     ,'st_flicensed_teach_pct'\n",
    "                     ,'st_short_susp_per_c_num'\n",
    "\n",
    "\n",
    "                     ]\n",
    "\n",
    "schData = schData.drop(ManualDropFeatures,axis=1)\n",
    "print('*********After: Removing fetures we are not interested in ******')\n",
    "schData.info(verbose=False)\n",
    "print ('\\r\\nFeatures Deleted: ', len(ManualDropFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These features have big number of the weight in feature selection, trying to remove them and see if it will be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schData = schData.drop(['Number of farm returns All','Number of farm returns 100KLT200K',\n",
    "# 'Number of farm returns LT25K',\n",
    "# 'Number of farm returns 25KLT50K',\n",
    "# 'Number of farm returns 50KLT75K',\n",
    "# 'Number of farm returns 75KLT100K',\n",
    "# 'Number of farm returns GE200K'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup the data set\n",
    "schData_bak = schData\n",
    "schData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "schData = shuffle(schData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X and y\n",
    "y= schData['_1yr_tchr_trnovr_pct']\n",
    "X = schData.drop(['_1yr_tchr_trnovr_pct'],axis=1)\n",
    "# save the header\n",
    "saved_cols = X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "# #Save as data frames\n",
    "# X = pd.DataFrame(X)\n",
    "# X.columns = saved_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####  Normalizing the Data\n",
    "* standardize data so that variance is 1 and mean is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit: https://fatihsarigoz.com/scaling-rfe.html\n",
    "# Transform\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_Stanardized = scaler.fit_transform(X)\n",
    "#Save as data frames\n",
    "X_Stanardized = pd.DataFrame(X_Stanardized)\n",
    "X_Stanardized.columns = saved_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* rescale range to (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rescale data (between 0 and 1)\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_reScaled =pd.DataFrame( scaler.fit_transform(X))\n",
    "X_reScaled = pd.DataFrame(X_reScaled)\n",
    "X_reScaled.columns = saved_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threthold for the feature selection\n",
    "max_features =200\n",
    "\n",
    "#schData = schData_bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit https://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/\n",
    "# https://www.kaggle.com/arthurtok/feature-ranking-rfe-random-forest-linear-models\n",
    "\n",
    "\n",
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso,ElasticNet)\n",
    "from sklearn.feature_selection import RFE, f_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from minepy import MINE\n",
    "\n",
    "np.random.seed(0)\n",
    "size = 750\n",
    "names = X.columns\n",
    "ranks = {}\n",
    "def rank_to_dict(ranks, names, order=1):\n",
    "    minmax = MinMaxScaler()\n",
    "    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n",
    "    ranks = map(lambda x: round(x, 2), ranks)\n",
    "    return dict(zip(names, ranks ))\n",
    "\n",
    "lr = LinearRegression(normalize=True)\n",
    "lr.fit(X_Stanardized, y)\n",
    "#ranks[\"Linear reg\"] = rank_to_dict(np.abs(lr.coef_), names)\n",
    "ranks[\"Linear reg\"] = rank_to_dict(lr.coef_, names)\n",
    "\n",
    "ridge = Ridge(alpha=7)\n",
    "ridge.fit(X_Stanardized, y)\n",
    "#ranks[\"Ridge\"] = rank_to_dict(np.abs(ridge.coef_), names)\n",
    "ranks[\"Ridge\"] = rank_to_dict(ridge.coef_, names)\n",
    "\n",
    "lasso = Lasso(alpha=.0001)\n",
    "lasso.fit(X_reScaled, y)\n",
    "#ranks[\"Lasso\"] = rank_to_dict(np.abs(lasso.coef_), names)\n",
    "ranks[\"Lasso\"] = rank_to_dict(lasso.coef_, names)\n",
    "\n",
    "elasticNet = ElasticNet(alpha=.0001)\n",
    "elasticNet.fit(X_reScaled, y)\n",
    "#ranks[\"ElasticNet\"] = rank_to_dict(np.abs(ElasticNet.coef_), names)\n",
    "ranks[\"ElasticNet\"] = rank_to_dict(elasticNet.coef_, names)\n",
    "\n",
    "\n",
    "# rlasso = RandomizedLasso(alpha=0.04)\n",
    "# rlasso.fit(X, Y)\n",
    "# ranks[\"Stability\"] = rank_to_dict(np.abs(rlasso.scores_), names)\n",
    "\n",
    "#stop the search when 5 features are left (they will get equal scores)\n",
    "rfe = RFE(lr, n_features_to_select=5)\n",
    "rfe.fit(X_Stanardized,y)\n",
    "ranks[\"RFE\"] = rank_to_dict(list(map(float, rfe.ranking_)), names, order=-1)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 100)\n",
    "rf.fit(X_Stanardized,y)\n",
    "ranks[\"RF\"] = rank_to_dict(rf.feature_importances_, names)\n",
    "\n",
    "\n",
    "f, pval = f_regression(X_Stanardized, y, center=True)\n",
    "ranks[\"Corr.\"] = rank_to_dict(f, names)\n",
    "\n",
    "\n",
    "mine = MINE(alpha=.05)\n",
    "mic_scores = []\n",
    "for i in range(X_Stanardized.shape[1]):\n",
    "    #print(i)\n",
    "    mine.compute_score(np.array(X_Stanardized)[:,i], y)\n",
    "    m = mine.mic()\n",
    "    mic_scores.append(m)\n",
    "ranks[\"MIC\"] = rank_to_dict(mic_scores, names) \n",
    "#print(ranks[\"MIC\"])\n",
    "\n",
    "r = {}\n",
    "for name in names:\n",
    "    r[name] = round(np.mean([ranks[method][name] for method in ranks.keys()]), 2)\n",
    "methods = sorted(ranks.keys())\n",
    "ranks[\"Mean\"] = r\n",
    "methods.append(\"Mean\")\n",
    "\n",
    "# print (\"\\t%s\" % \"\\t\".join(methods))\n",
    "# for name in names:\n",
    "#     print(\"%s\\t%s\" % (name, \"\\t\".join(map(str,[ranks[method][name] for method in methods]))))\n",
    "ranksDF = pd.DataFrame(ranks)\n",
    "ranksDF.sort_values(by='Mean', inplace=True, ascending=False )\n",
    "#ranksDF.columns = ['feature_names','Linear reg','Ridge','Lasso','ElasticNet','RFE','RF','Corr.','MIC','Mean']\n",
    "\n",
    "ranksDF.to_csv(\"RFE_Feature_Importance_Results_2017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranksDF.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranksDF['Mean'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanplot = pd.DataFrame(list(r.items()), columns= ['Feature','Mean Ranking'])\n",
    "\n",
    "# Sort the dataframe\n",
    "meanplot = meanplot.sort_values('Mean Ranking', ascending=False)\n",
    "meanplotTop50 = meanplot[0:50]\n",
    "# Let's plot the ranking of the features\n",
    "sns.catplot(x=\"Mean Ranking\", y=\"Feature\", data = meanplotTop50, kind=\"bar\", \n",
    "               height=20, aspect=1.9, palette='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_to_remove = ranksDF[ranksDF['Mean']<=.28].index\n",
    "# X = X.drop(feature_to_remove, axis=1)\n",
    "\n",
    "# #feature_to_remove\n",
    "# print ('\\r\\nFeatures Deleted: ', len(feature_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_remove = ranksDF[ranksDF['Mean']<=ranksDF['Mean'].mean()-ranksDF['Mean'].std()].index\n",
    "X_Stanardized = X_Stanardized.drop(feature_to_remove, axis=1)\n",
    "\n",
    "#feature_to_remove\n",
    "print ('\\r\\nFeatures Deleted: ', len(feature_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Stanardized.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "**Cross validation is performed using repeated holdout using ShuffleSplit()**\n",
    "* Ten folds are used\n",
    "* The split is: 90% training data and 10% test data\n",
    "* A random seed is set so the same random test and training splits are used each time cross validation is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide data into test and training splits\n",
    "#from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.10, random_state=12)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_Stanardized, y, test_size=0.1,random_state =12 )\n",
    "print(\"Training set\", X_train.shape, y_train.shape)\n",
    "print(\"Test set\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Scorers for Evaluating Regression Models \n",
    "\n",
    "**All regression models created in this notebook are validated using the following metrics:**\n",
    "* Mean Absolute Error (MAE)\n",
    "* Root Mean Squared Error (RMSE) - https://stackoverflow.com/questions/17197492/root-mean-square-error-in-python\n",
    "* Mean Absolute Percentage Error (MAPE) - https://stackoverflow.com/questions/42250958/how-to-optimize-mape-code-in-python\n",
    "\n",
    "**For details on making scorers to return multiple mean error scores see:**\n",
    "* http://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html\n",
    "* https://github.com/scikit-learn/scikit-learn/pull/7388\n",
    "* https://github.com/drorata/multiscorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it may not make sense to use MAPE in our project based on below articles:\n",
    "# http://www.catchbull.com/catchblogs/why-mape-doesnt-work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model Evaluation\n",
    "** All regression models are evaluated using the regression model evaluation function below: ** \n",
    "* The following regression evaluation function uses the cross validation object and the custom scorers in the two cells above in combination with sklearn.model_selection's cross_validate function to perform cross validation for regression estimators.\n",
    "* The cross validation object above uses a random seed to ensure that all regression estimators are tested on the same randomly selected records for each cross validation fold.\n",
    "* Custom scorers are created using the three chosen mean error scores and passed into cross_validate(), so all three scores are calcualted using a single call to cross_validate().\n",
    "* All of this functionality is wrapped within the custom EvaluateRegressionEstimator() function below so multiple regression models may be tested using the same test / train cv data and evaluation scores producing a consistent output for each model without the need to re-write the same code over and over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use mean absolute error (MAE) to score the regression models created \n",
    "#(the scale of MAE is identical to the response variable)\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error, r2_score \n",
    "from ml_metrics import rmse\n",
    "\n",
    "#from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error, r2_scorer \n",
    "\n",
    "#Function for Root mean squared error\n",
    "#https://stackoverflow.com/questions/17197492/root-mean-square-error-in-python\n",
    "# def rmse(y_actual, y_predicted):\n",
    "#     return np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Function for Mean Absolute Percentage Error (MAPE) - Untested\n",
    "#Adapted from - https://stackoverflow.com/questions/42250958/how-to-optimize-mape-code-in-python\n",
    "# revised from - https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn\n",
    "# still not working\n",
    "def mape(y_actual, y_predicted): \n",
    "    mask = y_actual != 0 \n",
    "    #print(y_actual)\n",
    "#     print(y_predicted)\n",
    "#     return (np.abs(y_actual - y_predicted)/y_actual)[mask].mean() * 100\n",
    "    y_actual, y_pred = np.array(y_actual), np.array(y_predicted)\n",
    "    return np.mean(np.abs((y_actual - y_pred) / y_actual))/len(y_actual) * 100\n",
    "    \n",
    "#Create scorers for rmse and mape functions\n",
    "mae_scorer = make_scorer(score_func=mean_absolute_error, greater_is_better=False)\n",
    "mse_scorer = make_scorer(score_func=mean_squared_error, greater_is_better=False)\n",
    "rmse_scorer = make_scorer(score_func=rmse, greater_is_better=False)\n",
    "mape_scorer = make_scorer(score_func=mape, greater_is_better=False)\n",
    "r2_scorer = make_scorer(score_func=r2_score, greater_is_better=True) \n",
    "\n",
    "#Make scorer array to pass into cross_validate() function for producing mutiple scores for each cv fold.\n",
    "errorScoring = {'MAE':  mae_scorer, \n",
    "                'MSE':  mse_scorer, \n",
    "                'RMSE': rmse_scorer,\n",
    "                'MAPE': mape_scorer,\n",
    "                'R2': r2_scorer       \n",
    "               } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def EvaluateRegressionEstimator(regEstimator, X, y, cv):\n",
    "    \n",
    "    scores = cross_validate(regEstimator, X, y, scoring=errorScoring, cv=cv, return_train_score=True)\n",
    "\n",
    "    #print(scores)\n",
    "    #cross val score sign-flips the outputs of MAE\n",
    "    # https://github.com/scikit-learn/scikit-learn/issues/2439\n",
    "    scores['test_MAE'] = scores['test_MAE'] * -1\n",
    "    scores['test_MSE'] = scores['test_MSE'] * -1\n",
    "    scores['test_MAPE'] = scores['test_MAPE'] * -1\n",
    "    scores['test_RMSE'] = scores['test_RMSE'] * -1\n",
    "\n",
    "    #print mean MAE for all folds \n",
    "    maeAvg = scores['test_MAE'].mean()\n",
    "    print_str = \"The average MAE for all cv folds is: \\t\\t\\t {maeAvg:.5}\"\n",
    "    print(print_str.format(maeAvg=maeAvg))\n",
    "\n",
    "     #print mean MAE for all folds \n",
    "    mseAvg = scores['test_MSE'].mean()\n",
    "    print_str = \"The average MSE for all cv folds is: \\t\\t\\t {mseAvg:.5}\"\n",
    "    print(print_str.format(mseAvg=mseAvg))\n",
    "    \n",
    "    #print mean test_MAPE for all folds\n",
    "    scores['test_MAPE'] = scores['test_MAPE']\n",
    "    mape_avg = scores['test_MAPE'].mean()\n",
    "    print_str = \"The average MAE percentage (MAPE) for all cv folds is: \\t {mape_avg:.5}\"\n",
    "    print(print_str.format(mape_avg=mape_avg))\n",
    "\n",
    "    #print mean MAE for all folds \n",
    "    RMSEavg = scores['test_RMSE'].mean()\n",
    "    print_str = \"The average RMSE for all cv folds is: \\t\\t\\t {RMSEavg:.5}\"\n",
    "    print(print_str.format(RMSEavg=RMSEavg))\n",
    "    print('*********************************************************')\n",
    "    \n",
    "    #print mean R-squared for all folds \n",
    "    R2avg = scores['test_R2'].mean()\n",
    "    print_str = \"The average R-Squared for all cv folds is: \\t\\t {R2avg:.5}\"\n",
    "    print(print_str.format(R2avg=R2avg))\n",
    "    print('*********************************************************')\n",
    "\n",
    "\n",
    "    print('Cross Validation Fold Mean Error Scores')\n",
    "    scoresResults = pd.DataFrame()\n",
    "    scoresResults['MAE'] = scores['test_MAE']\n",
    "    scoresResults['MSE'] = scores['test_MSE']\n",
    "    scoresResults['MAPE'] = scores['test_MAPE']\n",
    "    scoresResults['RMSE'] = scores['test_RMSE']\n",
    "    scoresResults['R2'] = scores['test_R2']\n",
    "    return scoresResults\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Regression Model\n",
    "\n",
    "**Linear Regression is used to create a baseline model.  Since linear regression may predict response variable values outside the range of the training data's response variable, we create a linear regression estimator with graduation rate predictions clipped 0% and 100%. For details see:**\n",
    "* http://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator \n",
    "* https://github.com/scikit-learn/scikit-learn/issues/6950\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "* https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/template.py\n",
    "* https://stackoverflow.com/questions/44234682/how-to-use-sklearn-when-target-variable-is-a-proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make new estimator compatible for use with GridSearchCV() and cross_validate()\n",
    "# -  Cap predict function for LinearRegression between 0 and 100\n",
    "# -  See: Roll your own estimator links above for details. \n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class CappedLinearRegression(LinearRegression):\n",
    "    def predict(self, X):\n",
    "        return np.clip(super(CappedLinearRegression, self).predict(X), 0, 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Grid Search\n",
    "** Here we perform a grid search testing 40 models to find the best parameters for our Linear Regression model based on Mean Absolute Error.  See more on parameter tuning with grid search here:**\n",
    "* http://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Linear Regression object and perform a grid search to find the best parameters\n",
    "linreg = LinearRegression()\n",
    "parameters = {'normalize':(True,False), 'fit_intercept':(True,False),'normalize': (True, False),}\n",
    "\n",
    "#Create a grid search object using the  \n",
    "linregGridSearch = GridSearchCV(estimator=linreg\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#linregGridSearch hyperparameter search to find the best combination of parameters for our data\n",
    "linregGridSearch.fit(X_Stanardized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the parameterization of the best estimator\n",
    "linregEstimator = linregGridSearch.best_estimator_\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "CappedLinear_Results = EvaluateRegressionEstimator(linregEstimator, X_Stanardized, y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linregEstimator = linregGridSearch.best_estimator_\n",
    "\n",
    "linregEstimator.fit(X_train, y_train)\n",
    "linregyhat = linregEstimator.predict(X_test)\n",
    "print(\"Yhat Max: \", linregyhat.max())\n",
    "print(explained_variance_score(y_test,linregyhat, multioutput='uniform_average'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linCompare = pd.DataFrame([np.array(linregyhat),np.array(y_test)]).T\n",
    "linCompare.columns=['linregyhat','y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"linregyhat\", y=\"y_test\",  data=linCompare,height=8.27, aspect=11.7/8.27)\n",
    "sns.despine()\n",
    "\n",
    "#plt.title(\"Figure 4. North Coralina Public School One Year Teacher Attrition Rate vs. LEA Total School Expenditure (2014-2017)\")\n",
    "plt.ylabel('y_test')\n",
    "plt.xlabel('linregyhat')\n",
    "plt.savefig('Figure2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Support Vector Regression\n",
    "**This model uses Support Vector Machines for regression of continuous variables (SVR). Please see documentation here:\"**\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "* http://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Linear regression object and perform a grid search to find the best parameters\n",
    "from sklearn.svm import SVR\n",
    "svr = SVR()\n",
    "\n",
    "#Set up SVR parameters to test (WARNING: Creates 320 models!!!) \n",
    "costs = [0.001, 0.1]\n",
    "defGamma = 1 / X.shape[1]  #This is the default value for the gamma parameter\n",
    "gammas = [defGamma, 0.1]\n",
    "kernels = [ 'linear', 'rbf']\n",
    "parameters = {'C': costs, 'gamma' : gammas, 'kernel': kernels}\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "svrGridSearch = GridSearchCV(estimator=svr\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "svrGridSearch.fit(X_Stanardized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the best estimator parameters\n",
    "svrGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVR\n",
    "\n",
    "#Create a regression estimator with best parameters for cross validation\n",
    "# regEstimator = SVR(C=0.001, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
    "#                    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "svrEstimator = svrGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "SVR_Results=EvaluateRegressionEstimator(svrEstimator, X_Stanardized, y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do we predict graduation rates greater than 100%?\n",
    "# regEstimator = SVR(C=0.001, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
    "#                    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "svrEstimator.fit(X_train, y_train)\n",
    "svr_yhat = svrEstimator.predict(X_test)\n",
    "print(\"Yhat Max: \", svr_yhat.max())\n",
    "print(explained_variance_score(y_test,svr_yhat, multioutput='uniform_average'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linCompare = pd.DataFrame([np.array(svr_yhat),np.array(y_test)]).T\n",
    "linCompare.columns=['svr_yhat','y_test']\n",
    "\n",
    "sns.lmplot(x=\"svr_yhat\", y=\"y_test\",  data=linCompare,height=8.27, aspect=11.7/8.27)\n",
    "sns.despine()\n",
    "\n",
    "#plt.title(\"Figure 4. North Coralina Public School One Year Teacher Attrition Rate vs. LEA Total School Expenditure (2014-2017)\")\n",
    "plt.ylabel('y_test')\n",
    "plt.xlabel('svr_yhat')\n",
    "plt.savefig('Figure2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Attrition Rate - Lasso Regression\n",
    "**This model uses Lasso regression (L1 Norm). Please see documentation here:\"**\n",
    "* **Caution!** - See documentation for fit_intercept, normalize, and copy_X. Lasso can over-write your X data!\n",
    "* Lasso may also perform scaling as well.  Please see docs!\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a regression object and perform a grid search to find the best parameters\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso()\n",
    "\n",
    "#Test parameters \n",
    "alpha = [0.0001,0.001, 0.1, 1, 10, 20]\n",
    "selection = ['cyclic','random']\n",
    "warm_start = [True, False]\n",
    "fit_intercept = [True, False]\n",
    "normalize = [True, False]\n",
    "copy_X = [True, False]\n",
    "parameters = {'alpha': alpha, 'selection': selection, 'warm_start': warm_start\n",
    "              ,'fit_intercept':fit_intercept,'normalize':normalize,'copy_X': copy_X}\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "lassoGridSearch = GridSearchCV(estimator=lasso\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "lassoGridSearch.fit(X_Stanardized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the best estimator parameters\n",
    "lassoGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a regression estimator with best parameters for cross validation\n",
    "lassoEstimator =lassoGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "Lasso_Results = EvaluateRegressionEstimator(lassoEstimator, X_Stanardized, y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do we predict graduation rates greater than 100%?\n",
    "# regEstimator = Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=10000,\n",
    "#    normalize=True, positive=False, precompute=True, random_state=0,\n",
    "#    selection='cyclic', tol=0.0001, warm_start=True)\n",
    "#lassoEstimator = lassoGridSearch.best_estimator_\n",
    "\n",
    "lassoEstimator.fit(X_train, y_train)\n",
    "Lasso_yhat = lassoEstimator.predict(X_test)\n",
    "print(\"Yhat Max: \", Lasso_yhat.max())\n",
    "print(explained_variance_score(y_test,Lasso_yhat, multioutput='uniform_average'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "**This model uses Ridge regression (L2 Norm). Please see documentation here:\"**\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a regression object and perform a grid search to find the best parameters\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge()\n",
    "\n",
    "#Test parameters \n",
    "alpha = [0.001, 0.1, 1, 5, 10, 20]\n",
    "solver = [ 'svd', 'cholesky', 'lsqr', 'sparse_cg']#, 'sag']#, 'saga']\n",
    "parameters = {'alpha': alpha, 'solver': solver}\n",
    "#parameters = {'alpha': alpha}\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "ridgeGridSearch = GridSearchCV(estimator=ridge\n",
    "                   , n_jobs=10 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "ridgeGridSearch.fit(X_Stanardized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the best estimator parameters\n",
    "ridgeGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Create a regression estimator with best parameters for cross validation\n",
    "ridgeEstimator = ridgeGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "Ridge_Results = EvaluateRegressionEstimator(ridgeEstimator, X_Stanardized, y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do we predict graduation rates greater than 100%?\n",
    "# regEstimator = Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=10000,\n",
    "#    normalize=True, random_state=0, solver='saga', tol=0.0001)\n",
    "ridgeEstimator = ridgeGridSearch.best_estimator_\n",
    "\n",
    "ridgeEstimator.fit(X_train, y_train)\n",
    "ridge_yhat = ridgeEstimator.predict(X_test)\n",
    "print(\"Yhat Max: \", ridge_yhat.max())\n",
    "print(explained_variance_score(y_test,ridge_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linCompare = pd.DataFrame([np.array(ridge_yhat),np.array(y_test)]).T\n",
    "linCompare.columns=['ridge_yhat','y_test']\n",
    "\n",
    "sns.lmplot(x=\"ridge_yhat\", y=\"y_test\",  data=linCompare,height=8.27, aspect=11.7/8.27)\n",
    "sns.despine()\n",
    "\n",
    "#plt.title(\"Figure 4. North Coralina Public School One Year Teacher Attrition Rate vs. LEA Total School Expenditure (2014-2017)\")\n",
    "plt.ylabel('y_test')\n",
    "plt.xlabel('ridge_yhat')\n",
    "plt.savefig('Figure2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net Regression\n",
    "**This model uses Elastic Net Regression (L1 and L2 Norm mixing). Please see documentation here:\"**\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a regression object and perform a grid search to find the best parameters\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elasticNet = ElasticNet(fit_intercept=True, normalize=True, precompute=True, copy_X=True\n",
    "          , max_iter=10000, tol=0.0001, random_state=0)\n",
    " \n",
    "#Test parameters\n",
    "l1_ratio = [0.001, 0.01, 0.1, 0.5, 0.75, 1]\n",
    "alpha = [0.001, 0.1, 1, 10]\n",
    "selection = ['cyclic','random']\n",
    "warm_start = [True, False]\n",
    "parameters = {'l1_ratio': l1_ratio, 'alpha': alpha, 'selection': selection, 'warm_start': warm_start}\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "elasticNetGridSearch = GridSearchCV(estimator=elasticNet\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "elasticNetGridSearch.fit(X_Stanardized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the best estimator parameters\n",
    "elasticNetGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a regression estimator with best parameters for cross validation\n",
    "elasticNetEstimator = elasticNetGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "elastic_Results = EvaluateRegressionEstimator(elasticNetEstimator, X_Stanardized, y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do we predict graduation rates greater than 100%?\n",
    "elasticNetEstimator =  elasticNetGridSearch.best_estimator_\n",
    "\n",
    "elasticNet = elasticNetEstimator.fit(X_train, y_train)\n",
    "elasticNet_yhat = elasticNetEstimator.predict(X_test)\n",
    "print(\"Yhat Max: \", elasticNet_yhat.max())\n",
    "print(explained_variance_score(y_test,elasticNet_yhat, multioutput='uniform_average'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linCompare = pd.DataFrame([np.array(elasticNet_yhat),np.array(y_test)]).T\n",
    "linCompare.columns=['elasticNet_yhat','y_test']\n",
    "\n",
    "sns.lmplot(x=\"elasticNet_yhat\", y=\"y_test\",  data=linCompare,height=8.27, aspect=11.7/8.27)\n",
    "sns.despine()\n",
    "\n",
    "#plt.title(\"Figure 4. North Coralina Public School One Year Teacher Attrition Rate vs. LEA Total School Expenditure (2014-2017)\")\n",
    "plt.ylabel('y_test')\n",
    "plt.xlabel('elasticNet_yhat')\n",
    "plt.savefig('Figure2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit: https://www.kaggle.com/mburakergenc/predictions-with-xgboost-and-linear-regression\n",
    "#credit: https://www.kaggle.com/stuarthallows/using-xgboost-with-scikit-learn\n",
    "# https://www.kaggle.com/jayatou/xgbregressor-with-gridsearchcv\n",
    "# Various hyper-parameters to tune\n",
    "# import xgboost as xgb\n",
    "# from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# xgb1 = XGBRegressor()\n",
    "# parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "#               'objective':['reg:linear'],\n",
    "#               'learning_rate': [.03, 0.05, .07], #so called `eta` value\n",
    "#               'max_depth': [5, 6, 7],\n",
    "#               'min_child_weight': [4],\n",
    "#               'silent': [1],\n",
    "#               'subsample': [0.7],\n",
    "#               'colsample_bytree': [0.7],\n",
    "#               'n_estimators': [500]}\n",
    "\n",
    "# xgbGridSearch = GridSearchCV(xgb1,\n",
    "#                         parameters,\n",
    "#                         cv = 2,\n",
    "#                         n_jobs = 5,\n",
    "#                         verbose=True)\n",
    "\n",
    "# xgbGridSearch.fit(X_Stanardized,\n",
    "#          y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the best estimator parameters\n",
    "# xgbGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create a regression estimator with best parameters for cross validation\n",
    "# xgbEstimator = xgbGridSearch.best_estimator_\n",
    "\n",
    "# #Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "# xgb_Results = EvaluateRegressionEstimator(xgbEstimator, X_Stanardized, y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Do we predict graduation rates greater than 100%?\n",
    "# xgbEstimator =  xgbGridSearch.best_estimator_\n",
    "\n",
    "# xgbEstimator.fit(X_train, y_train)\n",
    "# xgb_yhat = xgbEstimator.predict(X_test)\n",
    "# print(\"Yhat Max: \", xgb_yhat.max())\n",
    "# print(explained_variance_score(y_test,xgb_yhat, multioutput='uniform_average'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuals Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "\n",
    "# Instantiate the linear model and visualizer\n",
    "\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ridge = Ridge()\n",
    "visualizer = ResidualsPlot(ridge)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the model\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.poof()                 # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from yellowbrick.regressor import PredictionError\n",
    "\n",
    "# Instantiate the linear model and visualizer\n",
    "\n",
    "fig = plt.figure(figsize=(14, 14))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "\n",
    "visualizer = PredictionError(ridge)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge + XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle and split the data into training and testing subsets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_Stanardized, y, test_size=0.20, random_state=12)\n",
    "\n",
    "# Success\n",
    "print(\"Training set\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set\", X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit: https://www.kaggle.com/priyadwivedi/kernel-ridge-regression-and-xgboost\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def kaggle_rmse(true,pred):\n",
    "    mse = mean_squared_error(true, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def r2_metric(true,pred):\n",
    "    r2 = r2_score(true, pred)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Train on log of prices\n",
    "#y_train = np.log(y_train)\n",
    "\n",
    "# Train the model using the training sets\n",
    "lr.fit(X_train, y_train)\n",
    "pred_valid = lr.predict(X_valid)\n",
    "\n",
    "#pred_valid_exp = np.exp(pred_valid)\n",
    "#pred_linear = pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at whats been predicted\n",
    "print(y_valid.min(), y_valid.max(), y_valid.mean())\n",
    "print(pred_valid.min(), pred_valid.max(), pred_valid.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Kaggle Score: \", kaggle_rmse(y_valid,pred_valid))\n",
    "print(\"R2:\", r2_metric(y_valid,pred_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ridge Regression\n",
    "from sklearn import linear_model\n",
    "met = linear_model.RidgeCV(alphas=[0.1,0.5, 1.0, 10.0, 15.0, 20.0, 50.0])\n",
    "met.fit(X_train, y_train)\n",
    "pred_valid = met.predict(X_valid)\n",
    "\n",
    "#pred_valid_exp = np.exp(pred_valid)\n",
    "\n",
    "# Best alpha\n",
    "print(\"Best alpha:\" ,met.alpha_)\n",
    "pred_ridge = pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_valid.min(), y_valid.max(), y_valid.mean())\n",
    "print(pred_valid.min(), pred_valid.max(), pred_valid.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Kaggle Score: \", kaggle_rmse(y_valid,pred_valid))\n",
    "print(\"R2:\", r2_metric(y_valid,pred_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at important metrics\n",
    "coef = pd.Series(met.coef_, index = X_train.columns)\n",
    "imp_coef = pd.concat([coef.sort_values().head(20),\n",
    "                     coef.sort_values().tail(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Submit soln to Kaggle\n",
    "# pred_test_log = met.predict(df_test_final)\n",
    "# pred_test_log.shape\n",
    "# pred_test_log[:5]\n",
    "\n",
    "# pred_test_exp = np.exp(pred_test_log)\n",
    "# pred_test_exp[:5]\n",
    "\n",
    "# soln1 = pd.DataFrame({\"id\":test.Id, \"SalePrice\":pred_test_exp})\n",
    "# soln1.to_csv(\"ridge_reg.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients in the Ridge Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb; print(\"XGBoost\",xgb.__version__)\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.model_selection import ShuffleSplit, cross_validate\n",
    "from sklearn.model_selection import GridSearchCV   #Perforing grid search\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function to help fit chosen algorithm \n",
    "def modelfit(alg, useTrainCV=True, cv_folds=5, early_stopping_rounds=100):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "        xgtest = xgb.DMatrix(X_valid)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='rmse', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(X_train, y_train, eval_metric='rmse')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(X_train)\n",
    "    #dtrain_exp = np.exp(dtrain_predictions)\n",
    "    dvalid_predictions = alg.predict(X_valid)\n",
    "    #dvalid_exp = np.exp(dvalid_predictions)\n",
    "            \n",
    "    #Print model report:\n",
    "    print(\"Kaggle Score - Train \", kaggle_rmse(y_train, dtrain_predictions ))\n",
    "    print(\"Kaggle Score - Validation: \", kaggle_rmse(y_valid, dvalid_predictions ))\n",
    "    print(\"R2 - Validation:\", r2_metric(y_valid,dvalid_predictions))\n",
    "    \n",
    "    #print(pd.Series(alg.booster()))\n",
    "                       \n",
    "    feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#C Start with a learning rate = 0.1 and look at Kaggle score and no of features \n",
    "xgb1 = xgb.XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=500,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " #objective= 'reg:squarederror',\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "modelfit(xgb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "dtest = xgb.DMatrix(X_valid)\n",
    "params = {\"max_depth\":5, \"eta\":0.1, \"min_child_weight\":1,\n",
    " \"gamma\":0,\n",
    " \"subsample\":0.8,\n",
    " \"colsample_bytree\" :0.8,\n",
    " #\"objective\": 'reg:squarederror',\n",
    " \"scale_pos_weight\":1}\n",
    "model = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)\n",
    "model.loc[10:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best tuned \n",
    "# max_depth =3 , min_child_weight: 3, gamma = 0, learning_rate = 0.1, \n",
    "#subsample = 0.8, colsample = 0.9, n_estimators = 400\n",
    "\n",
    "xgb2 = xgb.XGBRegressor(\n",
    "     learning_rate = 0.1,\n",
    "     n_estimators=400,\n",
    "     max_depth= 3,\n",
    "     min_child_weight= 3,\n",
    "     gamma=0,\n",
    "     subsample=0.8,\n",
    "     colsample_bytree= 0.9,\n",
    "     #objective= 'reg:squarederror',\n",
    "     scale_pos_weight=1,\n",
    "     seed=27)\n",
    "modelfit(xgb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "dtest = xgb.DMatrix(X_valid)\n",
    "params = {\"max_depth\":3, \"eta\":0.1, \"min_child_weight\":3,\n",
    " \"gamma\":0,\n",
    " \"subsample\":0.8,\n",
    " \"colsample_bytree\" :0.9,\n",
    " #\"objective\": 'reg:squarederror',\n",
    " \"scale_pos_weight\":1}\n",
    "model = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)\n",
    "model.loc[5:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model - reduce learning rate and add a lot of estimators\n",
    "xgb4 = xgb.XGBRegressor(\n",
    "     learning_rate = 0.01,\n",
    "     n_estimators=5000,\n",
    "     max_depth= 3,\n",
    "     min_child_weight= 3,\n",
    "     gamma=0,\n",
    "     subsample=0.8,\n",
    "     colsample_bytree= 0.9,\n",
    "    # objective= 'reg:squarederror',\n",
    "     scale_pos_weight=1,\n",
    "     seed=27)\n",
    "modelfit(xgb4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best regressor\n",
    "model_xgb = xgb.XGBRegressor(\n",
    "     learning_rate = 0.01,\n",
    "     n_estimators=500,\n",
    "     max_depth= 5,\n",
    "     min_child_weight= 3,\n",
    "     gamma=0,\n",
    "     subsample=0.8,\n",
    "     colsample_bytree= 0.8,\n",
    "    # objective= 'reg:squarederror',\n",
    "     scale_pos_weight=1,\n",
    "     seed=27)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "xgb_preds = model_xgb.predict(X_valid)\n",
    "pred_xgb = xgb_preds\n",
    "pred_xgb[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_valid.min(), y_valid.max(), y_valid.mean())\n",
    "print(pred_xgb.min(), pred_xgb.max(), pred_xgb.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Kaggle Score: \", kaggle_rmse(y_valid,pred_xgb))\n",
    "print(\"R2:\", r2_metric(y_valid,pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the two predictions\n",
    "predictions = pd.DataFrame({\"xgb\":pred_xgb, \"ridge\":pred_ridge})\n",
    "predictions.plot(x = \"xgb\", y = \"ridge\", kind = \"scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the outcome\n",
    "fig, ax = plt.subplots()\n",
    "ri =ax.scatter(y_valid, pred_ridge, c='r')\n",
    "xgb =ax.scatter(y_valid, pred_xgb, c='b')\n",
    "ax.plot([-5,-1], [-5,-1], 'r-', lw=2)\n",
    "ax.set_xlabel('Actual value')\n",
    "ax.set_ylabel('Predicted value')\n",
    "ax.set_ylim([0, 0.5])\n",
    "ax.set_xlim([0, 0.5])\n",
    "plt.grid(True)\n",
    "plt.legend((ri, xgb),\n",
    "           ('Pred_Ridge', 'Pred XGB'),\n",
    "           scatterpoints=1,\n",
    "           loc='best',\n",
    "           ncol=3,\n",
    "           fontsize=8)\n",
    "fig.savefig('Results of Regressions')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
